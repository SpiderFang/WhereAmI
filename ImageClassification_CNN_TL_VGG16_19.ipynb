{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using Transfer learning to improve CNN accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 4, 4, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               2097408   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 15)                3855      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 15)                0         \n",
      "=================================================================\n",
      "Total params: 16,815,951\n",
      "Trainable params: 2,101,263\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "\n",
    "# TRAIN_IMG_DIR = \"/data/examples/may_the_4_be_with_u/where_am_i/train/\"\n",
    "# TEST_IMG_DIR = \"/data/examples/may_the_4_be_with_u/where_am_i/testset/\"\n",
    "TRAIN_IMG_DIR = \"./train/\"\n",
    "TEST_IMG_DIR = \"./testset/\"\n",
    "# VALID_IMG_DIR =\"./validation/\"\n",
    "\n",
    "train_samples = 2985 #total 2985 images in train_img_dir belonging to 15 classes\n",
    "test_samples = 1500 #1500 images in test_img_dir\n",
    "num_classes = 15 #target labels(ground truth), total 15 classes\n",
    "\n",
    "# image shapes\n",
    "img_width = 200\n",
    "img_height = 200\n",
    "channels = 3\n",
    "input_shape = (img_width, img_height, channels)\n",
    "\n",
    "batch_size = 16\n",
    "epochs = 100\n",
    "\n",
    "## Loading pre-trained VGG network\n",
    "from keras.applications import VGG16\n",
    "from keras.applications import VGG19\n",
    "\n",
    "## Create convolution base model from pre-trained network\n",
    "conv_base = VGG16(include_top = False, weights = \"imagenet\", input_shape = input_shape)\n",
    "conv_base = VGG19(include_top = False, weights = \"imagenet\", input_shape = input_shape)\n",
    "conv_base.trainable = False #freeze the conv_base network\n",
    "\n",
    "model = Sequential()\n",
    "model.add(conv_base)\n",
    "#fully-connected NN layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "# model.add(Dense(num_classes, activation=\"softmax\"))\n",
    "# !change Activation from keras to tf.nn.softmax, because TF version too old on Server!\n",
    "model.add(Dense(num_classes))\n",
    "import tensorflow as tf\n",
    "model.add(Activation(tf.nn.softmax))\n",
    "\n",
    "# opt_adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.01, amsgrad=False)\n",
    "model.compile(loss = \"categorical_crossentropy\",\n",
    "              optimizer = \"adam\",\n",
    "              metrics = [\"accuracy\"])\n",
    "\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2008 images belonging to 15 classes.\n",
      "Found 977 images belonging to 15 classes.\n",
      "Found 1500 images belonging to 1 classes.\n",
      "(128, 128, 3)\n",
      "(128, 128, 3)\n",
      "(128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "## Using Keras ImageDataGenerator to load images batch and do data augmentation on the fly.\n",
    "#!validation_split argument not support in Keras 2.1.3(server version)!\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale = 1./255,\n",
    "        rotation_range = 20,\n",
    "        width_shift_range = 0.2,\n",
    "        height_shift_range = 0.2,\n",
    "        shear_range = 0.2,\n",
    "        zoom_range = 0.2,\n",
    "        horizontal_flip = True,\n",
    "        validation_split = 0.33 \n",
    ")\n",
    "\n",
    "valid_datagen = ImageDataGenerator(\n",
    "        rescale = 1./255,\n",
    "#         rotation_range = 20,\n",
    "#         width_shift_range = 0.2,\n",
    "#         height_shift_range = 0.2,\n",
    "#         shear_range = 0.2,\n",
    "#         zoom_range = 0.2,\n",
    "#         horizontal_flip = True,\n",
    "        validation_split = 0.33\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        directory = TRAIN_IMG_DIR,\n",
    "        target_size = (img_width, img_height),\n",
    "        color_mode = \"rgb\",\n",
    "        batch_size = batch_size,\n",
    "        class_mode = \"categorical\",\n",
    "        shuffle = True,\n",
    "        seed = 33,\n",
    "        subset = \"training\"\n",
    ")\n",
    "\n",
    "validation_generator = valid_datagen.flow_from_directory(\n",
    "        directory = TRAIN_IMG_DIR,\n",
    "        target_size = (img_width, img_height),\n",
    "        color_mode = \"rgb\",\n",
    "        batch_size = batch_size,\n",
    "        class_mode = \"categorical\",\n",
    "        shuffle = True,\n",
    "        seed = 33,\n",
    "        subset = \"validation\"\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        directory = TEST_IMG_DIR,\n",
    "        target_size = (img_width, img_height),\n",
    "        color_mode = \"rgb\",\n",
    "        batch_size = 1,\n",
    "        class_mode = None,\n",
    "        shuffle = False,\n",
    ")\n",
    "\n",
    "## Amounts of individual set: training, validation, test\n",
    "# train_generator.n #amounts of training set\n",
    "# validation_generator.n #amounts of validation set\n",
    "# test_generator.n #amounts of test set\n",
    "\n",
    "## Labels from Keras data generator\n",
    "# print (train_generator.class_indices)\n",
    "# print (validation_generator.class_indices)\n",
    "\n",
    "## Image shape check\n",
    "print (train_generator.image_shape)\n",
    "print (validation_generator.image_shape)\n",
    "print (test_generator.image_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "125/125 [==============================] - 30s 242ms/step - loss: 1.9231 - acc: 0.3695 - val_loss: 1.1754 - val_acc: 0.6192\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.61924, saving model to ./checkpoint-01-1.18.hdf5\n",
      "Epoch 2/100\n",
      "125/125 [==============================] - 9s 73ms/step - loss: 1.4491 - acc: 0.4925 - val_loss: 0.9975 - val_acc: 0.6592\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.61924 to 0.65916, saving model to ./checkpoint-02-1.00.hdf5\n",
      "Epoch 3/100\n",
      "125/125 [==============================] - 9s 73ms/step - loss: 1.2643 - acc: 0.5585 - val_loss: 1.0036 - val_acc: 0.6745\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.65916 to 0.67451, saving model to ./checkpoint-03-1.00.hdf5\n",
      "Epoch 4/100\n",
      "125/125 [==============================] - 9s 73ms/step - loss: 1.1834 - acc: 0.5795 - val_loss: 0.9079 - val_acc: 0.6755\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.67451 to 0.67554, saving model to ./checkpoint-04-0.91.hdf5\n",
      "Epoch 5/100\n",
      "125/125 [==============================] - 9s 71ms/step - loss: 1.0922 - acc: 0.6100 - val_loss: 0.8131 - val_acc: 0.7277\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.67554 to 0.72774, saving model to ./checkpoint-05-0.81.hdf5\n",
      "Epoch 6/100\n",
      "125/125 [==============================] - 9s 72ms/step - loss: 1.0826 - acc: 0.6170 - val_loss: 0.8184 - val_acc: 0.7042\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.72774\n",
      "Epoch 7/100\n",
      "125/125 [==============================] - 9s 71ms/step - loss: 1.0564 - acc: 0.6230 - val_loss: 0.8622 - val_acc: 0.6940\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.72774\n",
      "Epoch 8/100\n",
      "125/125 [==============================] - 9s 72ms/step - loss: 0.9863 - acc: 0.6385 - val_loss: 0.7971 - val_acc: 0.7042\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.72774\n",
      "Epoch 9/100\n",
      "125/125 [==============================] - 9s 71ms/step - loss: 1.0011 - acc: 0.6420 - val_loss: 0.8019 - val_acc: 0.7073\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.72774\n",
      "Epoch 10/100\n",
      "125/125 [==============================] - 9s 69ms/step - loss: 0.9704 - acc: 0.6580 - val_loss: 0.7751 - val_acc: 0.7298\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.72774 to 0.72979, saving model to ./checkpoint-10-0.78.hdf5\n",
      "Epoch 11/100\n",
      "125/125 [==============================] - 9s 70ms/step - loss: 0.9823 - acc: 0.6490 - val_loss: 0.7791 - val_acc: 0.7206\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.72979\n",
      "Epoch 12/100\n",
      "125/125 [==============================] - 9s 69ms/step - loss: 0.9213 - acc: 0.6740 - val_loss: 0.7492 - val_acc: 0.7267\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.72979\n",
      "Epoch 13/100\n",
      "125/125 [==============================] - 9s 72ms/step - loss: 0.9253 - acc: 0.6840 - val_loss: 0.8387 - val_acc: 0.7062\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.72979\n",
      "Epoch 14/100\n",
      "125/125 [==============================] - 9s 69ms/step - loss: 0.9405 - acc: 0.6645 - val_loss: 0.8171 - val_acc: 0.7165\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.72979\n",
      "Epoch 15/100\n",
      "125/125 [==============================] - 9s 69ms/step - loss: 0.8928 - acc: 0.6900 - val_loss: 0.8056 - val_acc: 0.7165\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.72979\n",
      "Epoch 16/100\n",
      "125/125 [==============================] - 9s 68ms/step - loss: 0.8811 - acc: 0.6830 - val_loss: 0.7808 - val_acc: 0.7155\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.72979\n",
      "Epoch 17/100\n",
      "125/125 [==============================] - 9s 68ms/step - loss: 0.8750 - acc: 0.6905 - val_loss: 0.7373 - val_acc: 0.7400\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.72979 to 0.74002, saving model to ./checkpoint-17-0.74.hdf5\n",
      "Epoch 18/100\n",
      "125/125 [==============================] - 9s 68ms/step - loss: 0.8463 - acc: 0.6875 - val_loss: 0.7381 - val_acc: 0.7441\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.74002 to 0.74411, saving model to ./checkpoint-18-0.74.hdf5\n",
      "Epoch 19/100\n",
      "125/125 [==============================] - 8s 68ms/step - loss: 0.8256 - acc: 0.7070 - val_loss: 0.7679 - val_acc: 0.7369\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.74411\n",
      "Epoch 20/100\n",
      "125/125 [==============================] - 8s 67ms/step - loss: 0.8285 - acc: 0.7050 - val_loss: 0.6911 - val_acc: 0.7492\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.74411 to 0.74923, saving model to ./checkpoint-20-0.69.hdf5\n",
      "Epoch 21/100\n",
      "125/125 [==============================] - 9s 69ms/step - loss: 0.8549 - acc: 0.6875 - val_loss: 0.8083 - val_acc: 0.7236\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.74923\n",
      "Epoch 22/100\n",
      "125/125 [==============================] - 8s 68ms/step - loss: 0.8294 - acc: 0.6960 - val_loss: 0.7816 - val_acc: 0.7288\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.74923\n",
      "Epoch 23/100\n",
      "125/125 [==============================] - 8s 68ms/step - loss: 0.8044 - acc: 0.7105 - val_loss: 0.7304 - val_acc: 0.7503\n",
      "\n",
      "Epoch 00023: val_acc improved from 0.74923 to 0.75026, saving model to ./checkpoint-23-0.73.hdf5\n",
      "Epoch 24/100\n",
      "125/125 [==============================] - 9s 68ms/step - loss: 0.7986 - acc: 0.7260 - val_loss: 0.7572 - val_acc: 0.7462\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.75026\n",
      "Epoch 25/100\n",
      "125/125 [==============================] - 9s 69ms/step - loss: 0.8033 - acc: 0.7240 - val_loss: 0.7384 - val_acc: 0.7513\n",
      "\n",
      "Epoch 00025: val_acc improved from 0.75026 to 0.75128, saving model to ./checkpoint-25-0.74.hdf5\n",
      "Epoch 26/100\n",
      "125/125 [==============================] - 9s 68ms/step - loss: 0.7558 - acc: 0.7345 - val_loss: 0.7487 - val_acc: 0.7462\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.75128\n",
      "Epoch 27/100\n",
      "125/125 [==============================] - 9s 69ms/step - loss: 0.8039 - acc: 0.7370 - val_loss: 0.7491 - val_acc: 0.7472\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.75128\n",
      "Epoch 28/100\n",
      "125/125 [==============================] - 9s 69ms/step - loss: 0.7939 - acc: 0.7195 - val_loss: 0.7826 - val_acc: 0.7298\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.75128\n",
      "Epoch 29/100\n",
      "125/125 [==============================] - 8s 68ms/step - loss: 0.7799 - acc: 0.7315 - val_loss: 0.7430 - val_acc: 0.7380\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.75128\n",
      "Epoch 30/100\n",
      "125/125 [==============================] - 8s 67ms/step - loss: 0.7667 - acc: 0.7285 - val_loss: 0.7079 - val_acc: 0.7410\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.75128\n",
      "Epoch 00030: early stopping\n",
      "1500/1500 [==============================] - 7s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "## Fitting/Training the model\n",
    "# steps_per_epoch = train_samples // batch_size\n",
    "steps_per_epoch = train_generator.n // batch_size\n",
    "validation_steps = validation_generator.n // batch_size\n",
    "\n",
    "# Callbacks setting\n",
    "filepath = \"./checkpoint-{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
    "EarlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min')\n",
    "Checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=True, mode='max')\n",
    "Callback_list = [EarlyStopping, Checkpoint]\n",
    "\n",
    "history = model.fit_generator(\n",
    "                generator = train_generator,\n",
    "                steps_per_epoch = steps_per_epoch,\n",
    "                epochs = epochs,\n",
    "                callbacks = Callback_list,\n",
    "                validation_data = validation_generator,\n",
    "                validation_steps = validation_steps,\n",
    "#                 validation_data = None,\n",
    "#                 validation_steps = None,\n",
    "                shuffle = True\n",
    ")\n",
    "\n",
    "## Evaluate the model\n",
    "# model.evaluate_generator(generator = )\n",
    "\n",
    "## Predict the test set, then we'll get a probability nparray\n",
    "test_generator.reset()\n",
    "pred_probability = model.predict_generator(test_generator, verbose=1)\n",
    "\n",
    "## Convert the probability nparray to pandas dataframe to see its structure\n",
    "# df_pred = pd.DataFrame(pred_probability)\n",
    "# display(df_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the predicted class indices from model prediction result\n",
    "predicted_class_indices = np.argmax(pred_probability, axis=1)\n",
    "#default labels from Keras data generator\n",
    "keras_labels = (train_generator.class_indices)\n",
    "#get the names of class labels\n",
    "keras_labels_swap = dict((value, key) for key, value in keras_labels.items())\n",
    "class_name = [keras_labels_swap[idx] for idx in predicted_class_indices]\n",
    "\n",
    "## Reading pre-defined labels from mapping.txt, and store it to a dictionary\n",
    "mapping = {}\n",
    "with open(\"./mapping.txt\") as f:\n",
    "    for line in f:\n",
    "        (key, val) = line.split(sep=\",\")\n",
    "        mapping[str(key)] = int(val)\n",
    "\n",
    "## Because predicted_class_indices comes from Keras (data generator) default labels,\n",
    "## this is not our pre-defined labels (from mapping.txt).\n",
    "## I use pandas.Series.map(arg=Dict) to remap predicted_class_indices to pre-defined labels.\n",
    "ps = pd.Series(data = class_name)\n",
    "class_predictions = ps.map(mapping)\n",
    "\n",
    "## Save the results to a csv file\n",
    "#first, get filenames of all test images\n",
    "files = test_generator.filenames #this output will include the directory name!\n",
    "#use regular expression to retrieve exact filename of test images\n",
    "import re\n",
    "filenames = []\n",
    "for num in range(len(files)):\n",
    "    lst = re.findall(\"testimg/([a-zA-Z0-9]+).jpg\", files[num])\n",
    "    for idx, value in enumerate(lst):\n",
    "        filenames.append(value)\n",
    "\n",
    "#save the results to a csv file\n",
    "results = pd.DataFrame({\"id\" : filenames,\n",
    "                        \"class_name\" : class_name,\n",
    "                        \"class\" : class_predictions})\n",
    "results.to_csv(\"results.csv\", index=False)\n",
    "\n",
    "submission = pd.DataFrame({\"id\" : filenames,\n",
    "                           \"class\" : class_predictions})\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook ImageClassification_CNN.ipynb to script\n",
      "[NbConvertApp] Writing 7604 bytes to ImageClassification_CNN.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script ImageClassification_CNN.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
