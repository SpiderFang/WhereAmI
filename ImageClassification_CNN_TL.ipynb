{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using Transfer learning to improve CNN accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A local file was found, but it seems to be incomplete or outdated because the auto file hash does not match the original value of 4912a53fbd2a69346e7f2c0b5ec8c6d3 so we will re-download the data.\n",
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.8/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "30015488/30011760 [==============================] - 15s 0us/step\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "\n",
    "# TRAIN_IMG_DIR = \"/data/examples/may_the_4_be_with_u/where_am_i/train/\"\n",
    "# TEST_IMG_DIR = \"/data/examples/may_the_4_be_with_u/where_am_i/testset/\"\n",
    "TRAIN_IMG_DIR = \"./train/\"\n",
    "TEST_IMG_DIR = \"./testset/\"\n",
    "# VALID_IMG_DIR =\"./validation/\"\n",
    "\n",
    "train_samples = 2985 #total 2985 images in train_img_dir belonging to 15 classes\n",
    "test_samples = 1500 #1500 images in test_img_dir\n",
    "num_classes = 15 #target labels(ground truth), total 15 classes\n",
    "\n",
    "# image shapes\n",
    "img_width = 224\n",
    "img_height = 224\n",
    "channels = 3\n",
    "input_shape = (img_width, img_height, channels)\n",
    "\n",
    "batch_size = 16\n",
    "epochs = 100\n",
    "\n",
    "## Loading pre-trained network models in Keras\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.mobilenet import MobileNet\n",
    "from keras.applications.xception import Xception\n",
    "from keras.applications.densenet import DenseNet121\n",
    "# from keras.applications.densenet import DenseNet169\n",
    "# from keras.applications.densenet import DenseNet201\n",
    "\n",
    "## Setting pre-trained models\n",
    "# VGG16_model = VGG16(include_top = False, weights = \"imagenet\", input_shape = input_shape)\n",
    "# VGG19_model = VGG19(include_top = False, weights = \"imagenet\", input_shape = input_shape)\n",
    "# ResNet50_model = ResNet50(include_top = False, weights = \"imagenet\", input_shape = input_shape)\n",
    "# InceptionV3_model = InceptionV3(include_top = False, weights = \"imagenet\", input_shape = input_shape)\n",
    "# MobileNet_model = MobileNet(include_top = False, weights = \"imagenet\", input_shape = input_shape)\n",
    "# Xception_model = Xception(include_top = False, weights = \"imagenet\", input_shape = input_shape)\n",
    "DenseNet_model = DenseNet121(include_top = False, weights = \"imagenet\", input_shape = input_shape)\n",
    "\n",
    "#template:\n",
    "# conv_base = XXX_model\n",
    "conv_base = DenseNet_model\n",
    "conv_base.trainable = False #freeze the conv_base network\n",
    "\n",
    "## Create our model based on the pre-trained model\n",
    "model = Sequential()\n",
    "model.add(conv_base)\n",
    "\n",
    "#fully-connected NN layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "# model.add(Dense(num_classes, activation=\"softmax\"))\n",
    "# !change Activation from keras to tf.nn.softmax, because TF version too old on Server!\n",
    "model.add(Dense(num_classes))\n",
    "import tensorflow as tf\n",
    "model.add(Activation(tf.nn.softmax))\n",
    "\n",
    "# opt_adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.01, amsgrad=False)\n",
    "model.compile(loss = \"categorical_crossentropy\",\n",
    "              optimizer = \"adam\",\n",
    "              metrics = [\"accuracy\"])\n",
    "\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using Keras ImageDataGenerator to load images batch and do data augmentation on the fly.\n",
    "#!validation_split argument not support in Keras 2.1.3(server version)!\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale = 1./255,\n",
    "        rotation_range = 20,\n",
    "        width_shift_range = 0.2,\n",
    "        height_shift_range = 0.2,\n",
    "        shear_range = 0.2,\n",
    "        zoom_range = 0.2,\n",
    "        horizontal_flip = True,\n",
    "        validation_split = 0.33 \n",
    ")\n",
    "\n",
    "valid_datagen = ImageDataGenerator(\n",
    "        rescale = 1./255,\n",
    "#         rotation_range = 20,\n",
    "#         width_shift_range = 0.2,\n",
    "#         height_shift_range = 0.2,\n",
    "#         shear_range = 0.2,\n",
    "#         zoom_range = 0.2,\n",
    "#         horizontal_flip = True,\n",
    "        validation_split = 0.33\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        directory = TRAIN_IMG_DIR,\n",
    "        target_size = (img_width, img_height),\n",
    "        color_mode = \"rgb\",\n",
    "        batch_size = batch_size,\n",
    "        class_mode = \"categorical\",\n",
    "        shuffle = True,\n",
    "        seed = 33,\n",
    "        subset = \"training\"\n",
    ")\n",
    "\n",
    "validation_generator = valid_datagen.flow_from_directory(\n",
    "        directory = TRAIN_IMG_DIR,\n",
    "        target_size = (img_width, img_height),\n",
    "        color_mode = \"rgb\",\n",
    "        batch_size = batch_size,\n",
    "        class_mode = \"categorical\",\n",
    "        shuffle = True,\n",
    "        seed = 33,\n",
    "        subset = \"validation\"\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        directory = TEST_IMG_DIR,\n",
    "        target_size = (img_width, img_height),\n",
    "        color_mode = \"rgb\",\n",
    "        batch_size = 1,\n",
    "        class_mode = None,\n",
    "        shuffle = False,\n",
    ")\n",
    "\n",
    "## Amounts of individual set: training, validation, test\n",
    "# train_generator.n #amounts of training set\n",
    "# validation_generator.n #amounts of validation set\n",
    "# test_generator.n #amounts of test set\n",
    "\n",
    "## Labels from Keras data generator\n",
    "# print (train_generator.class_indices)\n",
    "# print (validation_generator.class_indices)\n",
    "\n",
    "## Image shape check\n",
    "print (train_generator.image_shape)\n",
    "print (validation_generator.image_shape)\n",
    "print (test_generator.image_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fitting/Training the model\n",
    "# steps_per_epoch = train_samples // batch_size\n",
    "steps_per_epoch = train_generator.n // batch_size\n",
    "validation_steps = validation_generator.n // batch_size\n",
    "\n",
    "# Callbacks setting\n",
    "filepath = \"./checkpoint-{epoch:02d}-{val_loss:.2f}-{val_acc:.2f}.hdf5\"\n",
    "EarlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min')\n",
    "Checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=True, mode='max')\n",
    "Callback_list = [EarlyStopping, Checkpoint]\n",
    "\n",
    "history = model.fit_generator(\n",
    "                generator = train_generator,\n",
    "                steps_per_epoch = steps_per_epoch,\n",
    "                epochs = epochs,\n",
    "                callbacks = Callback_list,\n",
    "                validation_data = validation_generator,\n",
    "                validation_steps = validation_steps,\n",
    "#                 validation_data = None,\n",
    "#                 validation_steps = None,\n",
    "                shuffle = True\n",
    ")\n",
    "\n",
    "## Evaluate the model\n",
    "# model.evaluate_generator(generator = )\n",
    "\n",
    "## Predict the test set, then we'll get a probability nparray\n",
    "test_generator.reset()\n",
    "pred_probability = model.predict_generator(test_generator, verbose=1)\n",
    "\n",
    "## Convert the probability nparray to pandas dataframe to see its structure\n",
    "# df_pred = pd.DataFrame(pred_probability)\n",
    "# display(df_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the predicted class indices from model prediction result\n",
    "predicted_class_indices = np.argmax(pred_probability, axis=1)\n",
    "#default labels from Keras data generator\n",
    "keras_labels = (train_generator.class_indices)\n",
    "#get the names of class labels\n",
    "keras_labels_swap = dict((value, key) for key, value in keras_labels.items())\n",
    "class_name = [keras_labels_swap[idx] for idx in predicted_class_indices]\n",
    "\n",
    "## Reading pre-defined labels from mapping.txt, and store it to a dictionary\n",
    "mapping = {}\n",
    "with open(\"./mapping.txt\") as f:\n",
    "    for line in f:\n",
    "        (key, val) = line.split(sep=\",\")\n",
    "        mapping[str(key)] = int(val)\n",
    "\n",
    "## Because predicted_class_indices comes from Keras (data generator) default labels,\n",
    "## this is not our pre-defined labels (from mapping.txt).\n",
    "## I use pandas.Series.map(arg=Dict) to remap predicted_class_indices to pre-defined labels.\n",
    "ps = pd.Series(data = class_name)\n",
    "class_predictions = ps.map(mapping)\n",
    "\n",
    "## Save the results to a csv file\n",
    "#first, get filenames of all test images\n",
    "files = test_generator.filenames #this output will include the directory name!\n",
    "#use regular expression to retrieve exact filename of test images\n",
    "import re\n",
    "filenames = []\n",
    "for num in range(len(files)):\n",
    "    lst = re.findall(\"testimg/([a-zA-Z0-9]+).jpg\", files[num])\n",
    "    for idx, value in enumerate(lst):\n",
    "        filenames.append(value)\n",
    "\n",
    "#save the results to a csv file\n",
    "results = pd.DataFrame({\"id\" : filenames,\n",
    "                        \"class_name\" : class_name,\n",
    "                        \"class\" : class_predictions})\n",
    "results.to_csv(\"results.csv\", index=False)\n",
    "\n",
    "submission = pd.DataFrame({\"id\" : filenames,\n",
    "                           \"class\" : class_predictions})\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to script ImageClassification_CNN.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
